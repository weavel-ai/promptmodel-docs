# Methods

- `get_prompts` fetches published prompts from a specific `PromptModel`.
- `run`, `stream` fetches published prompts, runs LLM API call locally, logs data to the cloud, and returns the result.
- `run_and_parse`, `stream_and_parse` fetches published prompts, runs LLM API call locally, parses the result, logs data, then returns the parsed result.
- Asynchronous methods like `arun`, `astream`, `arun_and_parse`, `astream_and_parse` are also available.

## Error Handling

All methods of `PromptModel` do not raise error.

Instead, `error` and `error_log` attributes will be set if error occurs.

If you use LLM API call with error handling, your previous code will be like this.

```python
max_retry = 3
trial = 0
while trial < max_retry:
	try:
		res = openai.ChatCompletion.create(**kwargs)
		parsed_outputs = parser(res)
		break
	except Exception as e:
		print(e)
		trial += 1
print(parsed_outputs)
```

If you use `PromptModel`, you can handle error like this.

```python
max_retry = 3
trial = 0
while trial < max_retry:
	res = PromptModel("example").run_and_parse(kwargs)
	if res.error:
		print(res.error_log)
		trial += 1
		continue
	break
print(res.parsed_outputs)
```

## get_prompts

`get_prompts()` is a method for fetching published prompts from a specific `PromptModel`.

```python
prompts = PromptModel("say_hello").get_prompts()
print(prompts)
# [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Say hello"}]
```

## run

`run()` is a basic method for LLM calls.

You can use `run()` with inputs as Dictionary.

```python
res = PromptModel("extract_keyword").run({"text": "I like apple."})
print(res.raw_output)
# apple
```

You can also use `arun()` for asynchronous usage.

```python
res = await PromptModel("extract_keyword").arun({"text": "I like apple."})
print(res.raw_output)
# apple
```

## stream

`stream()` is a method for streaming LLM calls.
It is same with OpenAI API call with `stream=True`.

```python
res = PromptModel("start_conversation").stream()
for chunk in res:
	print(chunk.raw_output)
	# Hi
	# ,
	# how
	# are
	# you
	# ?
```

You can also use `astream()` for asynchronous call.

```python
res = await PromptModel("start_conversation").astream()
async for chunk in res:
	print(chunk.raw_output)
	# Hi
	# ,
	# how
	# are
	# you
	# ?
```

## parse

PromptModel can have `parsing_type` and `output_keys` attributes.

If you set `parsing_type` and `output_keys` when you engineer your `PromptModel`, output keys can be parsed automatically.

Response of promptmodel will be parsed automatically and saved in `parsed_outputs` attribute as `Dict[str, str]`.

### run_and_parse

```python
res = PromptModel("get_keyword_and_score").run_and_parse({"text": text})
print(res.parsed_outputs)
# {'keyword': 'apple', 'score': 0.9}
```

### stream_and_parse

```python
res = PromptModel("get_keyword_and_score").stream_and_parse({"text": text})
for chunk in res:
	print(chunk.parsed_outputs)
	# {'keyword': 'apple'}
	# {'score': 0}
	# {'score': .}
	# {'score': 9}
```

## Response Format

All of **PromptModel**'s methods except for `get_prompts` return either an `LLMResponse` or `LLMStreamResponse` object.

### LLMResponse

Here is the exact output dictionary and types of `LLMResponse`.

```python
{
	'api_response' : {
		'choices': [
			{
				'finish_reason': Optional[str],
				'index': int,
				'message': {
					'role': str,
					'content': str,
					'function_call' : Optional[Dict[str, Any]],
				}
			}
		],
		'created': str,
		'model': str,
		'usage': {
			'prompt_tokens': int,
			'completion_tokens': int,
			'total_tokens': int
		},
		'response_ms' : float
	} or None,
	'raw_output' : Optional[str],
	'parsed_outputs' : Optional[Dict[str, str]],
	'function_call' : Optional[Dict[str, Any]],
	'error' : Optional[bool],
	'error_log' : Optional[str]
}
```

The format of `LLMResponse.api_response` is equal to the format of OpenAI API response.
For example, the code blocks below will return the same output.

```python
import openai
from promptmodel import PromptModel


openai_response = openai.ChatCompletion.create(
	model="gpt-3.5-turbo",
	messages=[{"role" : "system", "content" : ""}, {"role" : "user", "content" : "Say Hello"}]
)
print(openai_response['choices'][0]['messages']['content'])
# Hello, my name is GPT-3.

promptmodel_response = PromptModel("say_hello").run()
print(promptmodel_response.api_response['choices'][0]['messages']['content'])
# Hello, my name is GPT-3.
```

You can also use `LLMResponse.raw_output` to get the content from the LLM call's response.

```python
promptmodel_response = PromptModel("say_hello").run()
print(promptmodel_response.raw_output)
# Hello, my name is GPT-3.
```

When you use methods such as `run_and_parse`, the parsed results can be found in `LLMResponse.parsed_outputs`.

```python
promptmodel_response = PromptModel("say_hello_COT").run_and_parse()
print(promptmodel_response.parsed_outputs)
# {"think" : "I will say Hello", "response" : "Hello"}
```

### LLMStreamResponse

Streaming methods like `stream()` or `stream_and_parse()` return **LLMStreamResponse**.

Here is the exact json output and type of `LLMStreamResponse`.

```python
{
	'api_response' : {
		'choices': [
			{
				'finish_reason': Optional[str],
				'index': int,
				'message': {
					'role': str,
					'content': str,
					'function_call' : Optional[Dict[str, Any]],
				}
			}
		],
		'created': str,
		'model': str,
		'usage': {
			'prompt_tokens': int,
			'completion_tokens': int,
			'total_tokens': int
		},
		'response_ms' : float
	} or None,
	'raw_output' : Optional[str],
	'parsed_outputs' : Optional[Dict[str, str]],
	'function_call' : Optional[Dict[str, Any]],
	'error' : Optional[bool],
	'error_log' : Optional[str]
}
```

The format of `LLMStreamResponse.api_response` is equal to the format of OpenAI API response.
For example, the code blocks below will return the same output.

```python
import openai
from promptmodel import PromptModel


openai_response = openai.ChatCompletion.create(
	model="gpt-3.5-turbo",
	messages=[{"role" : "system", "content" : ""}, {"role" : "user", "content" : "Say Hello"}],
	stream=True
)
for chunk in openai_response:
	print(chunk['choices'][0]['delta']['content'])
# Hello,
# my
# name
# is
# GPT-3.

promptmodel_response = PromptModel("say_hello").stream()
for chunk in promptmodel_response:
	print(chunk.api_response['choices'][0]['delta']['content'])
# Hello,
# my
# name
# is
# GPT-3.
```

You can also use `LLMStreamResponse.raw_output` to get the content from the LLM call's response.

```python
promptmodel_response = PromptModel("say_hello").stream()
for chunk in promptmodel_response:
	print(chunk.raw_output)
# Hello,
# my
# name
# is
# GPT-3.
```

Using `stream_and_parse` will return the parsed results in `LLMStreamResponse.parsed_outputs`.
The streamed LLM response will be parsed in real-time, streamed as Dictionary.

```python
promptmodel_response = PromptModel("say_hello_COT").stream_and_parse()
for chunk in promptmodel_response:
	print(chunk.parsed_outputs)
	# {"think" : "I"}
	# {"think" : "will"}
	# {"think" : "say"}
	# {"think" : "Hello."}
	# {"response" : "Hello"}
```
